{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# parser = argparse.ArgumentParser(description='Flickr8k')\n",
    "# parser.add_argument('-f', default='', type=str)\n",
    "\n",
    "# parser.add_argument('--embed_size', type=int, default=256, metavar='N', help='embedding size (default: 256)')\n",
    "# parser.add_argument('--hidden_size', type=int, default=256, metavar='N', help='hidden size (default: 256)')\n",
    "# parser.add_argument('--batch_size', type=int, default=24, metavar='N', help='batch size (default: 24)')\n",
    "# parser.add_argument('--lr', type=float, default=2e-3, help='initial learning rate (default: 2e-3)')\n",
    "# parser.add_argument('--optim', type=str, default='Adam', help='optimizer to use (default: Adam)')\n",
    "# parser.add_argument('--num_epochs', type=int, default=50, help='number of epochs (default: 50)')\n",
    "# parser.add_argument('--when_decay', type=int, default=30, help='when to decay learning rate (default: 30)')\n",
    "# parser.add_argument('--seed', type=int, default=2024, help='random seed')\n",
    "# parser.add_argument('--num_layers', type=int, default=6, help='number of Transformer decoder layers (default: 6)')\n",
    "# parser.add_argument('--nhead', type=int, default=8, help='number of heads in the Transformer decoder (default: 8)')\n",
    "# parser.add_argument('--dim_feedforward', type=int, default=2048, help='dimension of the feedforward network in Transformer (default: 2048)')\n",
    "\n",
    "import argparse\n",
    "\n",
    "args = {\n",
    "    'embed_size': 256,\n",
    "    'hidden_size': 256,\n",
    "    'batch_size': 24,\n",
    "    'lr': 2e-3,\n",
    "    'optim': 'Adam',\n",
    "    'num_epochs': 50,\n",
    "    'when_decay': 30,\n",
    "    'seed': 2024,\n",
    "    'num_layers': 6,\n",
    "    'nhead': 8,\n",
    "    'dim_feedforward': 2048\n",
    "}\n",
    "\n",
    "args = argparse.Namespace(**args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset.py\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "import pandas as pd\n",
    "import spacy\n",
    "from collections import Counter\n",
    "from torch.utils.data import Dataset\n",
    "from PIL import Image\n",
    "from torchvision.transforms import transforms\n",
    "\n",
    "# 确保先下载 Spacy 模型\n",
    "# 执行: python -m spacy download en_core_web_sm\n",
    "spacy_eng = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "class Vocabulary:\n",
    "    def __init__(self, freq_threshold, glove_vocab):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            freq_threshold (int): 词汇表中词汇的最低频率\n",
    "            glove_vocab (set): GloVe词汇表的集合\n",
    "        \"\"\"\n",
    "        # 初始化词汇表，包括特殊标记\n",
    "        self.itos = {0: \"<PAD>\", 1: \"<SOS>\", 2: \"<EOS>\", 3: \"<UNK>\"}\n",
    "        self.stoi = {\"<PAD>\": 0, \"<SOS>\": 1, \"<EOS>\": 2, \"<UNK>\": 3}\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.glove_vocab = glove_vocab\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.itos)\n",
    "\n",
    "    @staticmethod\n",
    "    def tokenizer_eng(text):\n",
    "        return [tok.text.lower() for tok in spacy_eng.tokenizer(text)]\n",
    "\n",
    "    def build_vocabulary(self, sentences):\n",
    "        idx = 4  # 从索引4开始，因为0-3已经被占用\n",
    "        frequency = Counter()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for word in self.tokenizer_eng(sentence):\n",
    "                if word in self.glove_vocab:\n",
    "                    frequency[word] += 1\n",
    "                    if frequency[word] == self.freq_threshold:\n",
    "                        self.itos[idx] = word\n",
    "                        self.stoi[word] = idx\n",
    "                        idx += 1\n",
    "\n",
    "    def numericalize(self, sentence):\n",
    "        tokenized_text = self.tokenizer_eng(sentence)\n",
    "        numericalized = []\n",
    "        for word in tokenized_text:\n",
    "            if word in self.stoi:\n",
    "                numericalized.append(self.stoi[word])\n",
    "            else:\n",
    "                # 词不在词汇表中，则忽略该词\n",
    "                continue\n",
    "        return numericalized\n",
    "\n",
    "\n",
    "class FlickrDataset(Dataset):\n",
    "    def __init__(self, \n",
    "                 root_dir=\"/root/mmml-proj/flickr8k/Images\", \n",
    "                 caption_path=\"/root/mmml-proj/flickr8k/captions.txt\", \n",
    "                 glove_path=\"/root/mmml-proj/glove/glove.6B.300d.txt\",\n",
    "                 freq_threshold=5, \n",
    "                 transform=None, \n",
    "                 max_length=50):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            root_dir (string): 图片所在的目录\n",
    "            caption_path (string): 包含 captions 的文件路径（应为已划分的train或val的csv）\n",
    "            glove_path (string): GloVe文件路径\n",
    "            freq_threshold (int): 词汇表中词汇的最低频率\n",
    "            transform (callable, optional): 可选的转换函数\n",
    "            max_length (int): captions 的固定长度\n",
    "        \"\"\"\n",
    "        self.freq_threshold = freq_threshold\n",
    "        self.transform = transform\n",
    "        self.root_dir = root_dir\n",
    "        self.max_length = max_length\n",
    "\n",
    "        # 读取 captions 文件\n",
    "        self.df = pd.read_csv(caption_path)\n",
    "\n",
    "        self.captions = self.df['caption']\n",
    "        self.images = self.df['image']\n",
    "\n",
    "        # 加载GloVe词汇表\n",
    "        self.glove_vocab = self.load_glove_vocab(glove_path)\n",
    "\n",
    "        # 构建词汇表\n",
    "        self.vocab = Vocabulary(freq_threshold, self.glove_vocab)\n",
    "        self.vocab.build_vocabulary(self.captions.tolist())\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.df)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        caption = self.captions[index]\n",
    "        image = self.images[index]\n",
    "        img_path = os.path.join(self.root_dir, image)\n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        # 数值化 caption，并移除不在词汇表中的词\n",
    "        numericalized_caption = [self.vocab.stoi[\"<SOS>\"]]\n",
    "        numericalized_caption += self.vocab.numericalize(caption)\n",
    "        numericalized_caption.append(self.vocab.stoi[\"<EOS>\"])\n",
    "\n",
    "        # 截断或填充到固定长度\n",
    "        if len(numericalized_caption) < self.max_length:\n",
    "            numericalized_caption += [self.vocab.stoi[\"<PAD>\"]] * (self.max_length - len(numericalized_caption))\n",
    "        else:\n",
    "            numericalized_caption = numericalized_caption[:self.max_length]\n",
    "\n",
    "        return img, torch.tensor(numericalized_caption)\n",
    "\n",
    "    @staticmethod\n",
    "    def load_glove_vocab(glove_path):\n",
    "        \"\"\"\n",
    "        加载GloVe词汇表。\n",
    "        \n",
    "        Args:\n",
    "            glove_path (str): GloVe文件路径\n",
    "        \n",
    "        Returns:\n",
    "            set: GloVe词汇表的集合\n",
    "        \"\"\"\n",
    "        print(\"Loading GloVe vocabulary...\")\n",
    "        glove_vocab = set()\n",
    "        with open(glove_path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                parts = line.strip().split()\n",
    "                word = parts[0]\n",
    "                glove_vocab.add(word.lower())\n",
    "        print(f\"Loaded {len(glove_vocab)} words from GloVe.\")\n",
    "        return glove_vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import argparse\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision.transforms import transforms\n",
    "from src.models import CNN2Transformer  # 修改为导入 CNN2Transformer\n",
    "from src.dataset import FlickrDataset\n",
    "from PIL import Image\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "import nltk\n",
    "from nltk.translate.bleu_score import corpus_bleu, SmoothingFunction\n",
    "\n",
    "class Collate:\n",
    "    def __init__(self, pad_value):\n",
    "        self.pad_value = pad_value\n",
    "    \n",
    "    def __call__(self, batch):\n",
    "        imgs = torch.stack([item[0] for item in batch], dim=0)  # [batch_size, C, H, W]\n",
    "        captions = torch.stack([item[1] for item in batch], dim=1).permute(1, 0)  # [batch_size, max_length]\n",
    "        return imgs, captions\n",
    "    \n",
    "def get_loader(root_dir=\"/root/mmml-proj/flickr8k/Images\", \n",
    "               caption_path=\"/root/mmml-proj/flickr8k/captions.txt\", \n",
    "               transform=None, \n",
    "               batch_size=48, \n",
    "               freq_threshold = 5, \n",
    "               num_workers=8, \n",
    "               shuffle=True, \n",
    "               pin_memory=True, \n",
    "               selecting_samples = 10000):\n",
    "    dataset = FlickrDataset(root_dir=root_dir,caption_path=caption_path, transform=transform, freq_threshold = freq_threshold, selecting_samples = selecting_samples)\n",
    "    pad_value = dataset.vocab.stoi[\"<PAD>\"]\n",
    "    loader = DataLoader(dataset=dataset, batch_size=batch_size, num_workers=num_workers, shuffle=True, pin_memory=True, collate_fn=Collate(pad_value), generator=torch.Generator(device='cpu'))\n",
    "    return loader, dataset\n",
    "\n",
    "transform = transforms.Compose(\n",
    "            [\n",
    "                transforms.Resize((224, 224)),\n",
    "                transforms.ToTensor(),\n",
    "                transforms.Normalize([0.485, 0.456, 0.406],\n",
    "                                        [0.229, 0.224, 0.225]) \n",
    "            ]\n",
    "        )\n",
    "\n",
    "train_loader, train_set = get_loader(caption_path=\"/root/mmml-proj/flickr8k/train_captions.csv\", transform=transform, freq_threshold=4, selecting_samples='all', batch_size=args.batch_size)\n",
    "val_loader, val_set = get_loader(caption_path=\"/root/mmml-proj/flickr8k/val_captions.csv\", transform=transform, freq_threshold=4, selecting_samples='all', batch_size=args.batch_size)\n",
    "loader, dataset = get_loader(transform=transform, freq_threshold=4, selecting_samples='all', batch_size=args.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'<PAD>': 0,\n",
       " '<SOS>': 1,\n",
       " '<EOS>': 2,\n",
       " '<UNK>': 3,\n",
       " 'a': 4,\n",
       " '.': 5,\n",
       " 'girl': 6,\n",
       " 'dog': 7,\n",
       " 'in': 8,\n",
       " 'the': 9,\n",
       " 'each': 10,\n",
       " 'other': 11,\n",
       " 'little': 12,\n",
       " 'of': 13,\n",
       " 'with': 14,\n",
       " 'on': 15,\n",
       " 'front': 16,\n",
       " 'rainbow': 17,\n",
       " 'is': 18,\n",
       " 'white': 19,\n",
       " 'and': 20,\n",
       " 'black': 21,\n",
       " 'man': 22,\n",
       " 'bench': 23,\n",
       " 'sitting': 24,\n",
       " 'hat': 25,\n",
       " 'glasses': 26,\n",
       " 'an': 27,\n",
       " 'climbing': 28,\n",
       " 'at': 29,\n",
       " 'child': 30,\n",
       " 'grass': 31,\n",
       " 'running': 32,\n",
       " 'wooden': 33,\n",
       " 'red': 34,\n",
       " 'to': 35,\n",
       " 'orange': 36,\n",
       " 'ball': 37,\n",
       " 'near': 38,\n",
       " 'street': 39,\n",
       " 'boy': 40,\n",
       " 'him': 41,\n",
       " 'brown': 42,\n",
       " 'through': 43,\n",
       " 'snow': 44,\n",
       " 'over': 45,\n",
       " 'wearing': 46,\n",
       " 'next': 47,\n",
       " 'wall': 48,\n",
       " 'rock': 49,\n",
       " 'are': 50,\n",
       " 'while': 51,\n",
       " 'water': 52,\n",
       " 'playing': 53,\n",
       " 'catch': 54,\n",
       " 'yellow': 55,\n",
       " 'field': 56,\n",
       " 'toy': 57,\n",
       " 'large': 58,\n",
       " 'it': 59,\n",
       " 'young': 60,\n",
       " 'green': 61,\n",
       " 'people': 62,\n",
       " 'edge': 63,\n",
       " ',': 64,\n",
       " 'by': 65,\n",
       " 'tree': 66,\n",
       " 'lake': 67,\n",
       " 'couple': 68,\n",
       " 'person': 69,\n",
       " 'two': 70,\n",
       " 'ice': 71,\n",
       " 'dogs': 72,\n",
       " 'beach': 73,\n",
       " 'blue': 74,\n",
       " 'frozen': 75,\n",
       " 'his': 76,\n",
       " 'mouth': 77,\n",
       " 'baby': 78,\n",
       " 'kayak': 79,\n",
       " 'up': 80,\n",
       " 'its': 81,\n",
       " 'lab': 82,\n",
       " 'hockey': 83,\n",
       " 'outside': 84,\n",
       " 'sits': 85,\n",
       " 'building': 86,\n",
       " 'men': 87,\n",
       " 'standing': 88,\n",
       " 'looking': 89,\n",
       " 'into': 90,\n",
       " 'covered': 91,\n",
       " 'three': 92,\n",
       " 'grassy': 93,\n",
       " 'stands': 94,\n",
       " 'skyscraper': 95,\n",
       " 'small': 96,\n",
       " 'something': 97,\n",
       " 'shirt': 98,\n",
       " 'boats': 99,\n",
       " 'woman': 100,\n",
       " 'looks': 101,\n",
       " 'another': 102,\n",
       " 'for': 103,\n",
       " 'holding': 104,\n",
       " 'hand': 105,\n",
       " 'jacket': 106,\n",
       " 'watches': 107,\n",
       " 'television': 108,\n",
       " 'sit': 109,\n",
       " 'group': 110,\n",
       " 'several': 111,\n",
       " 'skateboard': 112,\n",
       " 'jumping': 113,\n",
       " 'as': 114,\n",
       " 'behind': 115,\n",
       " 'down': 116,\n",
       " 'pigtails': 117,\n",
       " 'plays': 118,\n",
       " 'pillow': 119,\n",
       " 'fight': 120,\n",
       " 'children': 121,\n",
       " 'workers': 122,\n",
       " 'from': 123,\n",
       " 'construction': 124,\n",
       " 'one': 125,\n",
       " 'play': 126,\n",
       " 'side': 127,\n",
       " 'her': 128,\n",
       " 'blond': 129,\n",
       " 'hands': 130,\n",
       " 'about': 131,\n",
       " 'train': 132,\n",
       " 'shore': 133,\n",
       " 'jumps': 134,\n",
       " 'off': 135,\n",
       " 'board': 136,\n",
       " 'high': 137,\n",
       " 'diving': 138,\n",
       " 'pool': 139,\n",
       " '-': 140,\n",
       " 'runs': 141,\n",
       " 'overlooking': 142,\n",
       " 'hill': 143,\n",
       " 'skier': 144,\n",
       " 'after': 145,\n",
       " 'laying': 146,\n",
       " 'object': 147,\n",
       " 'leaps': 148,\n",
       " 'air': 149,\n",
       " 'watching': 150,\n",
       " 'park': 151,\n",
       " 'balloons': 152,\n",
       " 'crowd': 153,\n",
       " 'head': 154,\n",
       " 'rides': 155,\n",
       " 'bike': 156,\n",
       " 'around': 157,\n",
       " 'night': 158,\n",
       " 'table': 159,\n",
       " 'shorts': 160,\n",
       " 'swimming': 161,\n",
       " 'inflatable': 162,\n",
       " 'slide': 163,\n",
       " 'throwing': 164,\n",
       " 'out': 165,\n",
       " 'camera': 166,\n",
       " 'boys': 167,\n",
       " 'fire': 168,\n",
       " 'face': 169,\n",
       " 'dock': 170,\n",
       " 'floor': 171,\n",
       " 'ride': 172,\n",
       " 'outdoors': 173,\n",
       " 'four': 174,\n",
       " 'kids': 175,\n",
       " 'together': 176,\n",
       " 'colored': 177,\n",
       " 'walking': 178,\n",
       " 'making': 179,\n",
       " 'their': 180,\n",
       " 'top': 181,\n",
       " 'mountain': 182,\n",
       " 'road': 183,\n",
       " 'fishing': 184,\n",
       " 'some': 185,\n",
       " 'horse': 186,\n",
       " 'snowy': 187,\n",
       " 'horses': 188,\n",
       " 'rope': 189,\n",
       " 'onto': 190,\n",
       " 'climbs': 191,\n",
       " 'stand': 192,\n",
       " 'light': 193,\n",
       " 'ground': 194,\n",
       " 'mountains': 195,\n",
       " 'hikers': 196,\n",
       " 'dress': 197,\n",
       " 'chalk': 198,\n",
       " 'tunnel': 199,\n",
       " 'dirt': 200,\n",
       " 'along': 201,\n",
       " 'fence': 202,\n",
       " 'sandy': 203,\n",
       " 'posing': 204,\n",
       " 'being': 205,\n",
       " 'hose': 206,\n",
       " 'smiling': 207,\n",
       " 'underwater': 208,\n",
       " 'smiles': 209,\n",
       " 'cliff': 210,\n",
       " 'snowmobiles': 211,\n",
       " \"'s\": 212,\n",
       " 'riding': 213,\n",
       " 'atv': 214,\n",
       " 'wheeler': 215,\n",
       " 'structure': 216,\n",
       " 'tan': 217,\n",
       " 'fountain': 218,\n",
       " 'city': 219,\n",
       " 'walks': 220,\n",
       " 'soccer': 221,\n",
       " 'picture': 222,\n",
       " 'painting': 223,\n",
       " 'bicycle': 224,\n",
       " 'get': 225,\n",
       " 'there': 226,\n",
       " 'kid': 227,\n",
       " 'gun': 228,\n",
       " 'frisbee': 229,\n",
       " 'many': 230,\n",
       " 'catches': 231,\n",
       " 'rocks': 232,\n",
       " 'rocky': 233,\n",
       " 'has': 234,\n",
       " 'striped': 235,\n",
       " 'climber': 236,\n",
       " 'helmet': 237,\n",
       " 'purple': 238,\n",
       " 'pants': 239,\n",
       " 'sculpture': 240,\n",
       " 'eats': 241,\n",
       " 'bird': 242,\n",
       " 'seeds': 243,\n",
       " 'sand': 244,\n",
       " 'ocean': 245,\n",
       " 'splashing': 246,\n",
       " 'ears': 247,\n",
       " 'stick': 248,\n",
       " 'open': 249,\n",
       " 'pink': 250,\n",
       " 'bed': 251,\n",
       " 'guy': 252,\n",
       " 'holds': 253,\n",
       " 'surrounded': 254,\n",
       " 'trees': 255,\n",
       " 'toddler': 256,\n",
       " 'dressed': 257,\n",
       " 'flag': 258,\n",
       " 'shirtless': 259,\n",
       " 'chair': 260,\n",
       " 'run': 261,\n",
       " 'surf': 262,\n",
       " 'pose': 263,\n",
       " 'carrying': 264,\n",
       " 'tennis': 265,\n",
       " 'splashes': 266,\n",
       " 'river': 267,\n",
       " 'pile': 268,\n",
       " 'big': 269,\n",
       " 'deck': 270,\n",
       " 'dry': 271,\n",
       " 'railing': 272,\n",
       " 'backpacks': 273,\n",
       " 'middle': 274,\n",
       " 'background': 275,\n",
       " 'way': 276,\n",
       " 'nearby': 277,\n",
       " 'skiers': 278,\n",
       " 'sidewalk': 279,\n",
       " 'suit': 280,\n",
       " 'shoes': 281,\n",
       " 'them': 282,\n",
       " 'bridge': 283,\n",
       " 'across': 284,\n",
       " 'whilst': 285,\n",
       " 'colorful': 286,\n",
       " 'painted': 287,\n",
       " 'costume': 288,\n",
       " 'traffic': 289,\n",
       " 'getting': 290,\n",
       " 'busy': 291,\n",
       " 'car': 292,\n",
       " 'women': 293,\n",
       " 'waiting': 294,\n",
       " 'bikes': 295,\n",
       " 'pulling': 296,\n",
       " 'that': 297,\n",
       " 'carriage': 298,\n",
       " 'net': 299,\n",
       " 'watch': 300,\n",
       " 'waves': 301,\n",
       " 'trumpet': 302,\n",
       " 'gear': 303,\n",
       " 'area': 304,\n",
       " 'takes': 305,\n",
       " 'old': 306,\n",
       " 'girls': 307,\n",
       " 'garden': 308,\n",
       " 'under': 309,\n",
       " 'tent': 310,\n",
       " 'grey': 311,\n",
       " 'nose': 312,\n",
       " 'ring': 313,\n",
       " 'video': 314,\n",
       " 'dark': 315,\n",
       " 'different': 316,\n",
       " 'intersection': 317,\n",
       " 'blonde': 318,\n",
       " 'jeans': 319,\n",
       " 'parade': 320,\n",
       " 'he': 321,\n",
       " 'all': 322,\n",
       " 'baseball': 323,\n",
       " 'blanket': 324,\n",
       " 'terrier': 325,\n",
       " 'lady': 326,\n",
       " 'station': 327,\n",
       " 'unicycle': 328,\n",
       " 'scooter': 329,\n",
       " 'reading': 330,\n",
       " 'tall': 331,\n",
       " 'eating': 332,\n",
       " 'walk': 333,\n",
       " 'lays': 334,\n",
       " 'arms': 335,\n",
       " 'picnic': 336,\n",
       " 'above': 337,\n",
       " 'taking': 338,\n",
       " 'family': 339,\n",
       " 'break': 340,\n",
       " 'climbers': 341,\n",
       " 'boat': 342,\n",
       " 'ledge': 343,\n",
       " 'yard': 344,\n",
       " 'back': 345,\n",
       " 'swing': 346,\n",
       " 'food': 347,\n",
       " 'spoon': 348,\n",
       " 'fallen': 349,\n",
       " 'sleeping': 350,\n",
       " 'playground': 351,\n",
       " 'beer': 352,\n",
       " 'hats': 353,\n",
       " 'wear': 354,\n",
       " 'look': 355,\n",
       " 'wood': 356,\n",
       " 'set': 357,\n",
       " 'sunglasses': 358,\n",
       " 'wetsuit': 359,\n",
       " 'towards': 360,\n",
       " 'who': 361,\n",
       " 'trying': 362,\n",
       " 'bubbles': 363,\n",
       " 'biting': 364,\n",
       " 'gray': 365,\n",
       " 'ear': 366,\n",
       " 'going': 367,\n",
       " 'leaping': 368,\n",
       " 'shakes': 369,\n",
       " 'bathing': 370,\n",
       " 'reflection': 371,\n",
       " 'asian': 372,\n",
       " 'stroller': 373,\n",
       " 'distance': 374,\n",
       " 'wooded': 375,\n",
       " 'lies': 376,\n",
       " 'trail': 377,\n",
       " 'biker': 378,\n",
       " 'path': 379,\n",
       " 'long': 380,\n",
       " 'haired': 381,\n",
       " 'cat': 382,\n",
       " 'adult': 383,\n",
       " 'police': 384,\n",
       " 'motorcycle': 385,\n",
       " 'outfit': 386,\n",
       " 'hanging': 387,\n",
       " 'pairs': 388,\n",
       " 'beside': 389,\n",
       " 'stream': 390,\n",
       " 'bicycles': 391,\n",
       " 'overalls': 392,\n",
       " 't': 393,\n",
       " 'outdoor': 394,\n",
       " 'outstretched': 395,\n",
       " 'others': 396,\n",
       " 'crossing': 397,\n",
       " 'alone': 398,\n",
       " 'bright': 399,\n",
       " 'drinking': 400,\n",
       " 'paved': 401,\n",
       " 'drink': 402,\n",
       " 'climb': 403,\n",
       " 'playpen': 404,\n",
       " 'coat': 405,\n",
       " 'landscape': 406,\n",
       " 'lying': 407,\n",
       " 'football': 408,\n",
       " 'gathered': 409,\n",
       " 'public': 410,\n",
       " 'sprinkler': 411,\n",
       " 'beautiful': 412,\n",
       " 'sun': 413,\n",
       " 'course': 414,\n",
       " 'ready': 415,\n",
       " 'golf': 416,\n",
       " 'backpack': 417,\n",
       " 'against': 418,\n",
       " 'flip': 419,\n",
       " 'pulled': 420,\n",
       " 'vest': 421,\n",
       " 'fair': 422,\n",
       " 'pond': 423,\n",
       " 'fish': 424,\n",
       " 'tongue': 425,\n",
       " 'bar': 426,\n",
       " 'sticking': 427,\n",
       " 'pull': 428,\n",
       " 'sled': 429,\n",
       " 'rider': 430,\n",
       " 'past': 431,\n",
       " 'harness': 432,\n",
       " 'sky': 433,\n",
       " 'attempting': 434,\n",
       " 'crouches': 435,\n",
       " 'drawing': 436,\n",
       " 'brick': 437,\n",
       " 'sweatshirt': 438,\n",
       " '\"': 439,\n",
       " 'american': 440,\n",
       " 'sweater': 441,\n",
       " 'swung': 442,\n",
       " 'room': 443,\n",
       " 'life': 444,\n",
       " 'cellphone': 445,\n",
       " 'shallow': 446,\n",
       " 'trunks': 447,\n",
       " 'pavement': 448,\n",
       " 'cap': 449,\n",
       " 'hair': 450,\n",
       " 'dirty': 451,\n",
       " 'writing': 452,\n",
       " 'hot': 453,\n",
       " 'uniforms': 454,\n",
       " 'umbrella': 455,\n",
       " 'clothes': 456,\n",
       " 'bars': 457,\n",
       " 'very': 458,\n",
       " 'sliding': 459,\n",
       " 'poses': 460,\n",
       " 'female': 461,\n",
       " 'cars': 462,\n",
       " 'tank': 463,\n",
       " 'empty': 464,\n",
       " 'railroad': 465,\n",
       " 'tracks': 466,\n",
       " 'sign': 467,\n",
       " 'peace': 468,\n",
       " 'wears': 469,\n",
       " 'store': 470,\n",
       " 'flowers': 471,\n",
       " 'five': 472,\n",
       " 'vehicle': 473,\n",
       " 'race': 474,\n",
       " 'carries': 475,\n",
       " 'swims': 476,\n",
       " 'smile': 477,\n",
       " 'laughing': 478,\n",
       " 'pier': 479,\n",
       " 'bucket': 480,\n",
       " 'body': 481,\n",
       " 'doing': 482,\n",
       " 'handstand': 483,\n",
       " 'trampoline': 484,\n",
       " 'backyard': 485,\n",
       " 'collar': 486,\n",
       " 'someone': 487,\n",
       " 'stone': 488,\n",
       " 'puppies': 489,\n",
       " 'sunny': 490,\n",
       " 'chasing': 491,\n",
       " 'balloon': 492,\n",
       " 'floating': 493,\n",
       " 'closeup': 494,\n",
       " 'eyes': 495,\n",
       " 'clouds': 496,\n",
       " 'swimsuit': 497,\n",
       " 'bikini': 498,\n",
       " 'foreground': 499,\n",
       " 'sprayed': 500,\n",
       " 'flat': 501,\n",
       " 'surface': 502,\n",
       " 'older': 503,\n",
       " 'benches': 504,\n",
       " 'adults': 505,\n",
       " 'rest': 506,\n",
       " 'art': 507,\n",
       " 'setting': 508,\n",
       " 'sticks': 509,\n",
       " 'asleep': 510,\n",
       " 'take': 511,\n",
       " 'cross': 512,\n",
       " 'lit': 513,\n",
       " 'this': 514,\n",
       " 'pharmacy': 515,\n",
       " 'same': 516,\n",
       " 'cheek': 517,\n",
       " 'both': 518,\n",
       " 'subway': 519,\n",
       " 'inside': 520,\n",
       " 'brightly': 521,\n",
       " 'swinging': 522,\n",
       " 'facing': 523,\n",
       " 'swings': 524,\n",
       " 'feet': 525,\n",
       " 'forest': 526,\n",
       " 'shirts': 527,\n",
       " 'teddy': 528,\n",
       " 'bear': 529,\n",
       " 'hiker': 530,\n",
       " 'scarves': 531,\n",
       " 'pole': 532,\n",
       " 'pushing': 533,\n",
       " 'tie': 534,\n",
       " 'concrete': 535,\n",
       " 'hydrant': 536,\n",
       " 'saxophone': 537,\n",
       " 'midair': 538,\n",
       " 'jumped': 539,\n",
       " 'foam': 540,\n",
       " 'statue': 541,\n",
       " 'liberty': 542,\n",
       " 'lot': 543,\n",
       " 'balls': 544,\n",
       " 'leash': 545,\n",
       " 'wet': 546,\n",
       " 'woods': 547,\n",
       " 'log': 548,\n",
       " 'elderly': 549,\n",
       " 'line': 550,\n",
       " 'hood': 551,\n",
       " 'jack': 552,\n",
       " 'using': 553,\n",
       " 'flags': 554,\n",
       " 'door': 555,\n",
       " 'hooded': 556,\n",
       " 'away': 557,\n",
       " 'during': 558,\n",
       " 'game': 559,\n",
       " 'softball': 560,\n",
       " 'thrown': 561,\n",
       " 'day': 562,\n",
       " 'number': 563,\n",
       " 'driving': 564,\n",
       " 'seven': 565,\n",
       " 'leaves': 566,\n",
       " 'waterfall': 567,\n",
       " 'filled': 568,\n",
       " 'jump': 569,\n",
       " 'tire': 570,\n",
       " 'between': 571,\n",
       " 'balcony': 572,\n",
       " 'view': 573,\n",
       " 'paddling': 574,\n",
       " 'canoe': 575,\n",
       " 'shop': 576,\n",
       " 'wine': 577,\n",
       " 'house': 578,\n",
       " 'headphones': 579,\n",
       " 'computer': 580,\n",
       " 'snowboarder': 581,\n",
       " 'trick': 582,\n",
       " 'snowboard': 583,\n",
       " 'window': 584,\n",
       " 'display': 585,\n",
       " 'track': 586,\n",
       " 'suits': 587,\n",
       " 'jackets': 588,\n",
       " 'talking': 589,\n",
       " 'resting': 590,\n",
       " 'below': 591,\n",
       " 'bag': 592,\n",
       " 'purse': 593,\n",
       " 'phone': 594,\n",
       " 'right': 595,\n",
       " 'clothing': 596,\n",
       " 'coffee': 597,\n",
       " 'go': 598,\n",
       " 'cart': 599,\n",
       " 'swim': 600,\n",
       " 'underwear': 601,\n",
       " 'seat': 602,\n",
       " 'seaweed': 603,\n",
       " 'coming': 604,\n",
       " 'sale': 605,\n",
       " 'bending': 606,\n",
       " 'faces': 607,\n",
       " 'star': 608,\n",
       " 'shaped': 609,\n",
       " 'like': 610,\n",
       " 'upside': 611,\n",
       " 'or': 612,\n",
       " 'gold': 613,\n",
       " 'luggage': 614,\n",
       " 'chases': 615,\n",
       " 'skis': 616,\n",
       " 'innertube': 617,\n",
       " 'waits': 618,\n",
       " 'boots': 619,\n",
       " 'furry': 620,\n",
       " 'attached': 621,\n",
       " 'shaking': 622,\n",
       " 'male': 623,\n",
       " 'eye': 624,\n",
       " 'flower': 625,\n",
       " 'arm': 626,\n",
       " 'couch': 627,\n",
       " 'bat': 628,\n",
       " 'player': 629,\n",
       " 'base': 630,\n",
       " 'dune': 631,\n",
       " 'these': 632,\n",
       " 'cement': 633,\n",
       " 'volleyball': 634,\n",
       " 'moving': 635,\n",
       " 'silver': 636,\n",
       " 'amusement': 637,\n",
       " 'make': 638,\n",
       " 'drinks': 639,\n",
       " 'trunk': 640,\n",
       " 'infant': 641,\n",
       " 'reaches': 642,\n",
       " 'cigarette': 643,\n",
       " 'mohawk': 644,\n",
       " 'does': 645,\n",
       " 'ninja': 646,\n",
       " 'leaning': 647,\n",
       " 'relaxes': 648,\n",
       " 'metal': 649,\n",
       " 'tube': 650,\n",
       " 'legs': 651,\n",
       " 'catching': 652,\n",
       " 'begins': 653,\n",
       " 'equipment': 654,\n",
       " 'they': 655,\n",
       " 'toward': 656,\n",
       " 'event': 657,\n",
       " 'wait': 658,\n",
       " 'skateboarder': 659,\n",
       " 'tray': 660,\n",
       " 'hamburgers': 661,\n",
       " 'bald': 662,\n",
       " 'which': 663,\n",
       " 'short': 664,\n",
       " 'restaurant': 665,\n",
       " 'eat': 666,\n",
       " 'lap': 667,\n",
       " 'parking': 668,\n",
       " 'curb': 669,\n",
       " 'no': 670,\n",
       " 'hiking': 671,\n",
       " 'plastic': 672,\n",
       " 'shoulders': 673,\n",
       " 'plain': 674,\n",
       " 'motorbike': 675,\n",
       " 'creek': 676,\n",
       " 'flying': 677,\n",
       " 'valley': 678,\n",
       " 'steep': 679,\n",
       " 'scaling': 680,\n",
       " 'slides': 681,\n",
       " 'stuffed': 682,\n",
       " 'carpet': 683,\n",
       " 'centipede': 684,\n",
       " 'low': 685,\n",
       " 'enjoys': 686,\n",
       " 'sports': 687,\n",
       " 'puppy': 688,\n",
       " 'branch': 689,\n",
       " 'plant': 690,\n",
       " 'cup': 691,\n",
       " 'shown': 692,\n",
       " 'hang': 693,\n",
       " 'playfully': 694,\n",
       " 'tries': 695,\n",
       " 'rough': 696,\n",
       " 'bicyclists': 697,\n",
       " 'piece': 698,\n",
       " 'bread': 699,\n",
       " 'band': 700,\n",
       " 'performing': 701,\n",
       " 'audience': 702,\n",
       " 'school': 703,\n",
       " 'leans': 704,\n",
       " 'suspended': 705,\n",
       " 'makes': 706,\n",
       " 'closed': 707,\n",
       " 'glass': 708,\n",
       " 'retrieving': 709,\n",
       " 'having': 710,\n",
       " 'rapids': 711,\n",
       " 'waters': 712,\n",
       " 'kayaking': 713,\n",
       " 'truck': 714,\n",
       " 'safety': 715,\n",
       " 'vests': 716,\n",
       " 'stuck': 717,\n",
       " 'jungle': 718,\n",
       " 'gym': 719,\n",
       " 'uniform': 720,\n",
       " 'new': 721,\n",
       " 'york': 722,\n",
       " 'monkey': 723,\n",
       " 'left': 724,\n",
       " 'goggles': 725,\n",
       " 'uses': 726,\n",
       " 'fighting': 727,\n",
       " 'golden': 728,\n",
       " 'wrestle': 729,\n",
       " 'skiing': 730,\n",
       " 'poles': 731,\n",
       " 'preparing': 732,\n",
       " 'kayaker': 733,\n",
       " 'rafting': 734,\n",
       " 'paddles': 735,\n",
       " 'barefoot': 736,\n",
       " 'lone': 737,\n",
       " 'ramp': 738,\n",
       " 'obstacle': 739,\n",
       " 'muddy': 740,\n",
       " 'mud': 741,\n",
       " 'stop': 742,\n",
       " 'button': 743,\n",
       " 'she': 744,\n",
       " 'can': 745,\n",
       " 'box': 746,\n",
       " 'scuba': 747,\n",
       " 'diver': 748,\n",
       " 'hallway': 749,\n",
       " 'ladies': 750,\n",
       " 'few': 751,\n",
       " 'lawn': 752,\n",
       " 'sunset': 753,\n",
       " 'have': 754,\n",
       " 'kissing': 755,\n",
       " 'put': 756,\n",
       " 'screen': 757,\n",
       " 'denim': 758,\n",
       " 'sheep': 759,\n",
       " 'stones': 760,\n",
       " 'pictures': 761,\n",
       " 'points': 762,\n",
       " ';': 763,\n",
       " 'bite': 764,\n",
       " 'animal': 765,\n",
       " 'skirt': 766,\n",
       " 'foot': 767,\n",
       " 'fly': 768,\n",
       " 'kite': 769,\n",
       " 'bank': 770,\n",
       " 'overpass': 771,\n",
       " 'pirate': 772,\n",
       " 'end': 773,\n",
       " 'wading': 774,\n",
       " 'pointing': 775,\n",
       " 'terrain': 776,\n",
       " 'plants': 777,\n",
       " 'curly': 778,\n",
       " 'skull': 779,\n",
       " 'performs': 780,\n",
       " 'beige': 781,\n",
       " 'seated': 782,\n",
       " 'reads': 783,\n",
       " 'book': 784,\n",
       " 'kicking': 785,\n",
       " 'goal': 786,\n",
       " 'just': 787,\n",
       " 'skinny': 788,\n",
       " 'camping': 789,\n",
       " 'patch': 790,\n",
       " 'hay': 791,\n",
       " 'necklace': 792,\n",
       " 'fluffy': 793,\n",
       " 'hind': 794,\n",
       " 'hugging': 795,\n",
       " 'spraying': 796,\n",
       " 'says': 797,\n",
       " 'cricket': 798,\n",
       " 'club': 799,\n",
       " 'guitar': 800,\n",
       " 'bath': 801,\n",
       " 'singing': 802,\n",
       " 'feeding': 803,\n",
       " 'snowball': 804,\n",
       " 'pack': 805,\n",
       " 'ski': 806,\n",
       " 'tents': 807,\n",
       " 'showing': 808,\n",
       " 'pulls': 809,\n",
       " 'artificial': 810,\n",
       " 'buildings': 811,\n",
       " 'team': 812,\n",
       " 'jewelry': 813,\n",
       " 'partially': 814,\n",
       " 'drives': 815,\n",
       " 'racer': 816,\n",
       " 'prepares': 817,\n",
       " 'country': 818,\n",
       " 'spotted': 819,\n",
       " 'spots': 820,\n",
       " 'boardwalk': 821,\n",
       " 'fenced': 822,\n",
       " 'lean': 823,\n",
       " 'crowded': 824,\n",
       " 'seats': 825,\n",
       " 'cut': 826,\n",
       " 'stump': 827,\n",
       " 'almost': 828,\n",
       " 'teeth': 829,\n",
       " 'hoop': 830,\n",
       " 'turn': 831,\n",
       " 'ropes': 832,\n",
       " 'sheer': 833,\n",
       " 'khaki': 834,\n",
       " 'motorcyclist': 835,\n",
       " 'deep': 836,\n",
       " 'racing': 837,\n",
       " 'lines': 838,\n",
       " 'collie': 839,\n",
       " 'platform': 840,\n",
       " 'fishes': 841,\n",
       " 'walkway': 842,\n",
       " 'van': 843,\n",
       " 'mirror': 844,\n",
       " 'onlookers': 845,\n",
       " 'hits': 846,\n",
       " 'camouflage': 847,\n",
       " 'huge': 848,\n",
       " 'tri': 849,\n",
       " 'enjoying': 850,\n",
       " 'alongside': 851,\n",
       " 'wide': 852,\n",
       " 'goes': 853,\n",
       " 'multicolored': 854,\n",
       " 'parked': 855,\n",
       " 'wakeboarding': 856,\n",
       " 'waterskiing': 857,\n",
       " 'jet': 858,\n",
       " 'laughs': 859,\n",
       " 'gets': 860,\n",
       " 'photo': 861,\n",
       " 'meal': 862,\n",
       " 'stairs': 863,\n",
       " 'flowery': 864,\n",
       " 'fur': 865,\n",
       " 'overhang': 866,\n",
       " 'steps': 867,\n",
       " 'trotting': 868,\n",
       " 'gravel': 869,\n",
       " 'cowboy': 870,\n",
       " 'goalie': 871,\n",
       " 'row': 872,\n",
       " 'ladder': 873,\n",
       " 'not': 874,\n",
       " 'spray': 875,\n",
       " 'string': 876,\n",
       " 'sniffing': 877,\n",
       " 'smaller': 878,\n",
       " 'sniffs': 879,\n",
       " 'gas': 880,\n",
       " 'pouring': 881,\n",
       " 'handle': 882,\n",
       " 'courtyard': 883,\n",
       " 'kneeling': 884,\n",
       " 'staring': 885,\n",
       " 'rug': 886,\n",
       " 'hold': 887,\n",
       " 'slipper': 888,\n",
       " 'matching': 889,\n",
       " 'dresses': 890,\n",
       " 'outfits': 891,\n",
       " 'firefighter': 892,\n",
       " 'try': 893,\n",
       " 'flowered': 894,\n",
       " 'leashes': 895,\n",
       " 'kiss': 896,\n",
       " 'strollers': 897,\n",
       " 'pushed': 898,\n",
       " 'bushes': 899,\n",
       " 'wakeboard': 900,\n",
       " 'spiderman': 901,\n",
       " 'cloudy': 902,\n",
       " 'costumed': 903,\n",
       " 'mask': 904,\n",
       " \"'\": 905,\n",
       " 'devil': 906,\n",
       " 'microphone': 907,\n",
       " 'sings': 908,\n",
       " 'jeep': 909,\n",
       " 'makeup': 910,\n",
       " 'heavy': 911,\n",
       " 'hoodie': 912,\n",
       " 'bikers': 913,\n",
       " 'float': 914,\n",
       " 'mouths': 915,\n",
       " 'held': 916,\n",
       " 'mickey': 917,\n",
       " 'mouse': 918,\n",
       " 'leaf': 919,\n",
       " 'fall': 920,\n",
       " 'wrestling': 921,\n",
       " 'bites': 922,\n",
       " 'only': 923,\n",
       " 'chairs': 924,\n",
       " 'lined': 925,\n",
       " 'helps': 926,\n",
       " 'helping': 927,\n",
       " 'spectators': 928,\n",
       " 'pig': 929,\n",
       " 'beam': 930,\n",
       " 'kayaks': 931,\n",
       " 'rows': 932,\n",
       " 'paddle': 933,\n",
       " 'item': 934,\n",
       " 'pit': 935,\n",
       " 'third': 936,\n",
       " 'gloves': 937,\n",
       " 'towel': 938,\n",
       " 'itself': 939,\n",
       " 'tug': 940,\n",
       " 'birthday': 941,\n",
       " 'party': 942,\n",
       " 'shows': 943,\n",
       " 'bags': 944,\n",
       " 'shoulder': 945,\n",
       " 'hangs': 946,\n",
       " 'mother': 947,\n",
       " 'before': 948,\n",
       " 'gate': 949,\n",
       " 'headed': 950,\n",
       " 'wagon': 951,\n",
       " 'tub': 952,\n",
       " 'corndogs': 953,\n",
       " 'shower': 954,\n",
       " 'shopping': 955,\n",
       " 'full': 956,\n",
       " 'cans': 957,\n",
       " 'traveling': 958,\n",
       " 'carts': 959,\n",
       " 'attempts': 960,\n",
       " 'kick': 961,\n",
       " 'socks': 962,\n",
       " 'flies': 963,\n",
       " 'wave': 964,\n",
       " 'surfboard': 965,\n",
       " 'sword': 966,\n",
       " 'martial': 967,\n",
       " 'arts': 968,\n",
       " 'canyon': 969,\n",
       " 'pauses': 970,\n",
       " 'jersey': 971,\n",
       " 'bay': 972,\n",
       " 'cloth': 973,\n",
       " 'growling': 974,\n",
       " 'turned': 975,\n",
       " 'backs': 976,\n",
       " 'smoking': 977,\n",
       " 'be': 978,\n",
       " 'basketball': 979,\n",
       " 'court': 980,\n",
       " 'style': 981,\n",
       " 'skating': 982,\n",
       " 'warm': 983,\n",
       " 'blow': 984,\n",
       " 'raft': 985,\n",
       " 'bowl': 986,\n",
       " 'medieval': 987,\n",
       " 'helmets': 988,\n",
       " 'biking': 989,\n",
       " 'wire': 990,\n",
       " 'shaggy': 991,\n",
       " 'paper': 992,\n",
       " 'tag': 993,\n",
       " 'puddle': 994,\n",
       " 'rain': 995,\n",
       " 'mountainside': 996,\n",
       " 'hike': 997,\n",
       " 'cleaning': 998,\n",
       " 'talk': 999,\n",
       " ...}"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.vocab.stoi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([3, 224, 224]), torch.Size([50]))"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "i=44\n",
    "train_set.__getitem__(i)[0].shape, train_set.__getitem__(i)[1].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "torch.Size([24, 3, 224, 224])\n",
      "torch.Size([24, 50])\n"
     ]
    }
   ],
   "source": [
    "print(len(next(enumerate(train_loader))[1]))\n",
    "img_batch = next(enumerate(train_loader))[1][0]\n",
    "print(img_batch.shape)\n",
    "caption_batch = next(enumerate(train_loader))[1][1]\n",
    "print(caption_batch.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 256])\n",
      "torch.Size([24, 256])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision.models as models\n",
    "import math\n",
    "\n",
    "class ImageEncoder(nn.Module):\n",
    "    \"\"\"使用ResNet18对图像进行编码\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size):\n",
    "        super(ImageEncoder, self).__init__()\n",
    "        resnet = models.resnet18(weights=models.ResNet18_Weights.IMAGENET1K_V1)\n",
    "        modules = list(resnet.children())[:-1]  # 移除最后的全连接层\n",
    "        self.resnet = nn.Sequential(*modules)\n",
    "        self.linear = nn.Linear(resnet.fc.in_features, embed_size)\n",
    "        self.bn = nn.BatchNorm1d(embed_size, momentum=0.01)\n",
    "\n",
    "    def forward(self, images):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            images: Tensor, shape [batch_size, 3, H, W]\n",
    "        Returns:\n",
    "            features: Tensor, shape [batch_size, embed_size]\n",
    "        \"\"\"\n",
    "        with torch.no_grad():\n",
    "            features = self.resnet(images)  # [batch_size, 512, 1, 1]\n",
    "        features = features.reshape(features.size(0), -1)  # [batch_size, 512]\n",
    "        features = self.linear(features)\n",
    "        # print(features.size())\n",
    "        features = self.bn(features)  # [batch_size, embed_size]\n",
    "        return features\n",
    "\n",
    "img_encoder = ImageEncoder(args.embed_size)\n",
    "print(img_encoder(img_batch).shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 256])\n",
      "torch.Size([24, 50, 3432])\n",
      "output:['stop', 'awning', 'hood', 'hood', 'cowboys', 'hood', 'watery', 'watery', 'hood', 'hood', 'watery', 'hood', 'hood', 'watery', 'watery', 'collar', 'hood', 'hood', 'collar', 'collar', 'collar', 'teaches', 'hood', 'collar', 'teaches', 'collar', 'collar', 'collar', 'hood', 'collar', 'collar', 'hood', 'restaurant', 'forward', 'hood', 'watery', 'hood', 'hood', 'watery', 'hood', 'restaurant', 'restaurant', 'hood', 'collar', 'hood', 'watery', 'watery', 'collar', 'collar', 'collar', 'glass', 'jogging', 'open', 'glass', 'blindfolds', 'glass', 'glass', 'glass', 'glass', 'open', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'enter', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'glass', 'spring', 'wild', 'suv', 'broken', 'spring', 'cries', 'dragged', 'dragged', 'suv', 'roll', 'dragged', 'lawn', 'dragged', 'wild', 'suv', 'dragged', 'dragged', 'wild', 'dragged', 'dragged', 'dragged', 'dragged', 'lawn', 'lifted', 'snows', 'dragged', 'wild', 'dragged', 'spring', 'dragged', 'wild', 'dragged', 'dragged', 'wild', 'snows', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'dragged', 'lawn', 'uncut', 'cafe', 'nails', 'uncut', 'cafe', 'purses', 'cafe', 'construction', 'spoon', 'uncut', 'cafe', 'cafe', 'artist', 'cafe', 'artist', 'sidelines', 'cafe', 'cafe', 'purses', 'cafe', 'cafe', 'uncut', 'uncut', 'uncut', 'track', 'plaid', 'cafe', 'uncut', 'construction', 'cafe', 'construction', 'cafe', 'uncut', 'cafe', 'hopscotch', 'artist', 'uncut', 'use', 'nails', 'artist', 'uncut', 'hopscotch', 'plaid', 'plaid', 'artist', 'uncut', 'uncut', 'hopscotch', 'uncut', 'hopscotch', 'machines', 'pointed', 'machines', 'spots', 'chains', 'motorcyclist', 'walker', 'walker', 'walker', 'chains', 'spots', 'pointed', 'boston', 'pointed', 'boston', 'machines', 'spots', 'spots', 'spots', 'spots', 'spots', 'spots', 'pointed', 'spots', 'spots', 'walker', 'spots', 'spots', 'spots', 'spots', 'spots', 'sparks', 'spots', 'spots', 'waterskies', 'machines', 'machines', 'machines', 'machines', 'spots', 'machines', 'spots', 'spots', 'spots', 'machines', 'spots', 'pointed', 'boston', 'spots', 'machines', 'begin', 'notes', 'squirts', 'squirts', 'begin', 'sticks', 'begin', 'squirts', 'squirts', 'begin', 'begin', 'squirts', 'begin', 'squirts', 'squirts', 'squirts', 'squirts', 'begin', 'begin', 'squirts', 'begin', 'squirts', 'begin', 'begin', 'squirts', 'squirts', 'begin', 'begin', 'ballerinas', 'begin', 'begin', 'begin', 'begin', 'begin', 'squirts', 'begin', 'begin', 'begin', 'squirts', 'begin', 'squirts', 'begin', 'begin', 'whilst', 'begin', 'begin', 'begin', 'begin', 'begin', 'score', 'gentleman', 'gentleman', 'icy', 'times', 'perform', 'times', 'perform', 'icy', 'bedspread', 'perform', 'perform', 'icy', 'perform', 'perform', 'saw', 'bedspread', 'icy', 'village', 'icy', 'bedspread', 'perform', 'perform', 'icy', 'icy', 'icy', 'icy', 'icy', 'bedspread', 'village', 'village', 'icy', 'icy', 'village', 'bedspread', 'perform', 'bedspread', 'icy', 'homemade', 'icy', 'icy', 'perform', 'bedspread', 'times', 'icy', 'icy', 'homemade', 'perform', 'bedspread', 'hole', 'icy', 'poses', 'poses', 'belt', 'belt', 'picket', 'belt', 'belt', 'picket', 'picket', 'belt', 'picket', 'belt', 'paints', 'poses', 'paints', 'picket', 'poses', 'poses', 'poses', 'belt', 'picket', 'belt', 'belt', 'picket', 'poses', 'poses', 'belt', 'picket', 'belt', 'poses', 'poses', 'belt', 'belt', 'picket', 'picket', 'picket', 'picket', 'picket', 'poses', 'poses', 'belt', 'belt', 'picket', 'belt', 'picket', 'picket', 'picket', 'picket', 'poses', 'poses', 'teenage', 'jean', 'jean', 'bounds', 'quilt', 'bounds', 'jean', 'jean', 'jean', 'bounds', 'take', 'jean', 'jean', 'bounds', 'teenage', 'bounds', 'bounds', 'bounds', 'jean', 'bounds', 'jean', 'bounds', 'teenage', 'jean', 'bounds', 'bounds', 'jean', 'bounds', 'bounds', 'bounds', 'bounds', 'teenage', 'jean', 'jean', 'jean', 'teenage', 'bounds', 'teenage', 'bounds', 'bounds', 'bounds', 'bounds', 'bounds', 'jean', 'bounds', 'bounds', 'jean', 'bounds', 'jean', 'hole', 'lines', 'lines', 'finger', 'festival', 'wheelchair', 'tiger', 'brightly', 'tire', 'finger', 'tire', 'adults', 'finger', 'lines', 'maneuver', 'finger', 'cigar', 'that', 'maneuver', 'maneuver', 'brightly', 'wings', 'that', 'festival', 'maneuver', 'wings', 'snows', 'adults', 'festival', 'finger', 'maneuver', 'maneuver', 'multiple', 'snows', 'cigar', 'maneuver', 'tiger', 'maneuver', 'snows', 'tiger', 'festival', 'snows', 'multiple', 'buried', 'casts', 'buried', 'snows', 'festival', 'snows', 'snows', 'snows', 'swoops', 'piled', 'piled', 'piled', 'swoops', 'swoops', 'piled', 'swoops', 'swoops', 'piled', 'swoops', 'swoops', 'piled', 'piled', 'elevator', 'swoops', 'swoops', 'swoops', 'swoops', 'saw', 'piled', 'piled', 'swoops', 'swoops', 'saw', 'saw', 'piled', 'swoops', 'swoops', 'hold', 'piled', 'swoops', 'swoops', 'hold', 'saw', 'swoops', 'hold', 'saw', 'piled', 'hold', 'piled', 'saw', 'saw', 'hold', 'saw', 'piled', 'swoops', 'hold', 'swoops', 'piled', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'pitching', 'pitching', 'underwear', 'pitching', 'pitching', 'underwear', 'underwear', 'pitching', 'pitching', 'snowmobiles', 'wears', 'wears', 'puffy', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'pitching', 'pitching', 'underwear', 'snowmobiles', 'underwear', 'snowmobiles', 'snowmobiles', 'underwear', 'underwear', 'uncut', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'snowmobiles', 'handbag', 'snowmobiles', 'snowmobiles', 'underwear', 'underwear', 'underwear', 'underwear', 'underwear', 'frame', 'has', 'viewer', 'have', 'frame', 'bunny', 'frame', 'works', 'parade', 'frame', 'has', 'outfits', 'frame', 'parade', 'parade', 'parade', 'parade', 'parade', 'parade', 'outfits', 'parade', 'parade', 'parade', 'parade', 'parade', 'has', 'has', 'parade', 'has', 'parade', 'has', 'has', 'has', 'parade', 'has', 'has', 'parade', 'has', 'has', 'frame', 'has', 'parade', 'parade', 'parade', 'parade', 'parade', 'frame', 'parade', 'parade', 'parade', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'next', 'harnessed', 'harnessed', 'harnessed', 'carry', 'clean', 'harnessed', 'harnessed', 'bowling', 'crocodile', 'branch', 'harnessed', 'crocodile', 'harnessed', 'harnessed', 'trench', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'carry', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'spotted', 'harnessed', 'carry', 'carry', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'harnessed', 'guard', 'harnessed', 'harnessed', 'harnessed', '!', '!', 'spraying', 'dolphins', '!', 'dolphins', 'catching', 'her', 'catching', 'catching', 'catching', 'catching', 'catching', '!', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'stuck', 'catching', 'records', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'catching', 'records', 'catching', 'rack', 'stuck', 'rack', 'catching', 'catching', 'catching', 'records', 'catching', 'wades', 'wades', 'line', 'wades', 'line', 'line', 'suv', 'suv', 'suv', 'wades', 'line', 'suv', 'suv', 'suv', 'waterskis', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'line', 'suv', 'taxi', 'suv', 'crowded', 'suv', 'suv', 'suv', 'line', 'line', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'suv', 'line', 'suv', 'suv', 'suv', 'guards', 'guards', 'concrete', 'suv', 'fleece', 'fleece', 'calf', 'flooded', 'calf', 'fleece', 'fleece', 'fleece', 'fleece', 'calf', 'fleece', 'driven', 'fleece', 'fleece', 'fleece', 'fleece', 'calf', 'fleece', 'fleece', 'calf', 'calf', 'calf', 'fleece', 'calf', 'spotted', 'fleece', 'driven', 'calf', 'calf', 'fleece', 'driven', 'calf', 'driven', 'calf', 'driven', 'silver', 'fleece', 'silver', 'calf', 'driven', 'calf', 'silver', 'dusty', 'silver', 'fleece', 'calf', 'fleece', 'calf', 'calf', 'calf', 'clean', 'trumpet', 'trumpet', 'collars', 'trumpet', 'trumpet', 'trumpet', 'trumpet', 'trumpet', 'trumpet', 'trumpet', 'hardwood', 'trumpet', 'trumpet', 'wheeler', 'little', 'fruit', 'fit', 'trumpet', 'fruit', 'hardwood', 'hardwood', 'trumpet', 'collars', 'fruit', 'fruit', 'hardwood', 'fruit', 'fruit', 'fruit', 'hardwood', 'bathrobe', 'collars', 'clean', 'wheeler', 'hardwood', 'hardwood', 'hardwood', 'trumpet', 'fruit', 'hardwood', 'clean', 'hardwood', 'hardwood', 'fruit', 'fruit', 'wheeler', 'fruit', 'fruit', 'fruit', 'border', 'falling', 'bicyclist', 'bicyclist', 'examining', 'bicyclist', 'dribbles', 'border', 'bicyclist', 'batsman', 'bicyclist', 'waterskies', 'bicyclist', 'dribbles', 'bicyclist', 'backpack', 'border', 'bicyclist', 'bicyclist', 'dribbles', 'backpack', 'backpack', 'bicyclist', 'bicyclist', 'bicyclist', 'backpack', 'bicyclist', 'bicyclist', 'bicyclist', 'dribbles', 'bicyclist', 'dribbles', 'bicyclist', 'bicyclist', 'shines', 'bicyclist', 'bicyclist', 'bicyclist', 'backpack', 'bicyclist', 'bicyclist', 'dribbles', 'bicyclist', 'bicyclist', 'bicyclist', 'bicyclist', 'bicyclist', 'bicyclist', 'bicyclist', 'bicyclist', 'mound', 'mound', 'mound', 'mound', 'mound', 'descending', 'mound', 'mound', 'mound', 'mound', 'mound', 'descending', 'mound', 'mound', 'mound', 'mound', 'mound', 'descending', 'mound', 'pose', 'mound', 'descending', 'pose', 'mound', 'mound', 'mound', 'mound', 'mound', 'pose', 'pose', 'mound', 'mound', 'pose', 'pose', 'mound', 'descending', 'mound', 'pose', 'pose', 'mound', 'pose', 'pose', 'mound', 'hallway', 'pose', 'mound', 'pose', 'mound', 'pose', 'pose', 'horns', 'brown', 'brown', 'maneuver', 'maneuver', 'brown', 'horns', 'brown', 'brown', 'maneuver', 'brown', 'brown', 'brown', 'horns', 'louis', 'maneuver', 'maneuver', 'horns', 'louis', 'horns', 'louis', 'brown', 'louis', 'brown', 'maneuver', 'louis', 'brown', 'louis', 'louis', 'maneuver', 'series', 'brown', 'louis', 'brown', 'maneuver', 'louis', 'louis', 'maneuver', 'maneuver', 'maneuver', 'maneuver', 'maneuver', 'louis', 'maneuver', 'use', 'louis', 'louis', 'brown', 'brown', 'louis', 'human', 'wrestlers', 'jumper', 'jumper', 'jumper', 'jumper', 'jumper', 'wrestlers', 'poised', 'jumper', 'single', 'jumper', 'single', 'jumper', 'jumper', 'jumper', 'jumper', 'haired', 'poised', 'sparks', 'poised', 'poised', 'rack', 'jumper', 'wrestlers', 'rack', 'sparks', 'wrestlers', 'poised', 'poised', 'jumper', 'sparks', 'wrestlers', 'poised', 'sparks', 'mountainside', 'sparks', 'jumper', 'mountainside', 'sparks', 'sparks', 'mountainside', 'poised', 'rack', 'jumper', 'jumper', 'jumper', 'sparks', 'jumper', 'jumper', 'surfs', 'surfs', 'walks', 'hugged', 'surfs', 'surfs', 'walks', 'hugged', 'hugged', 'surfs', 'walks', 'multicolor', 'surfs', 'walks', 'multicolor', 'walks', 'assistance', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'multicolor', 'walks', 'walks', 'walks', 'multicolor', 'walks', 'walks', 'multicolor', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'multicolor', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'walks', 'money', 'money', 'money', 'money', 'money', 'money', 'pillows', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'hood', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'clearing', 'money', 'money', 'money', 'money', 'money', 'hood', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money', 'money']\n"
     ]
    }
   ],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    \"\"\"为Transformer添加位置信息的模块\"\"\"\n",
    "    def __init__(self, embed_size, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        pe = torch.zeros(max_len, embed_size)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, embed_size, 2).float() * (-math.log(10000.0) / embed_size))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)  # [1, max_len, embed_size]\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            x: Tensor, shape [batch_size, seq_len, embed_size]\n",
    "        \"\"\"\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return x\n",
    "\n",
    "class TransformerDecoder(nn.Module):\n",
    "    \"\"\"使用Transformer解码器生成文本描述\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab_size, num_layers, nhead, dim_feedforward, dropout=0.1):\n",
    "        super(TransformerDecoder, self).__init__()\n",
    "        self.embed = nn.Embedding(vocab_size, embed_size)\n",
    "        self.positional_encoding = PositionalEncoding(embed_size)\n",
    "        decoder_layer = nn.TransformerDecoderLayer(d_model=embed_size, nhead=nhead, dim_feedforward=dim_feedforward, dropout=dropout)\n",
    "        self.transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.fc_out = nn.Linear(embed_size, vocab_size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.embed_size = embed_size\n",
    "\n",
    "    def forward(self, captions, memory):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            captions: Tensor, shape [batch_size, seq_len]\n",
    "            memory: Tensor, shape [batch_size, embed_size]\n",
    "        Returns:\n",
    "            outputs: Tensor, shape [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        caption_emb = self.embed(captions) * math.sqrt(self.embed_size)  # [batch_size, seq_len, embed_size]\n",
    "        caption_emb = self.positional_encoding(caption_emb)  # [batch_size, seq_len, embed_size]\n",
    "        caption_emb = caption_emb.permute(1, 0, 2)  # [seq_len, batch_size, embed_size]\n",
    "        # 将memory扩展为与Transformer期望的形状一致 [1, batch_size, embed_size]\n",
    "        memory = memory.unsqueeze(0)\n",
    "\n",
    "        tgt_mask = self.generate_square_subsequent_mask(caption_emb.size(0)).to(caption_emb.device)\n",
    "        # (caption_emb.size(), memory.size(), tgt_mask.size())\n",
    "        outputs = self.transformer_decoder(caption_emb, memory, tgt_mask=tgt_mask)  # [seq_len, batch_size, embed_size]\n",
    "        outputs = outputs.permute(1, 0, 2)  # [batch_size, seq_len, embed_size]\n",
    "        outputs = self.fc_out(outputs)  # [batch_size, seq_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"生成自回归的掩码\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "# 测试 TransformerDecoder\n",
    "decoder = TransformerDecoder(args.embed_size, len(dataset.vocab), args.num_layers, args.nhead, args.dim_feedforward)\n",
    "outputs = decoder(caption_batch, img_encoder(img_batch))\n",
    "print(outputs.size())\n",
    "outputs = outputs.reshape(-1, outputs.size(2))  # [(batch_size * (seq_len-1)), vocab_size]\n",
    "print(f'output:{[dataset.vocab.itos[idx.item()] for idx in outputs.argmax(dim=1)]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3432\n",
      "torch.Size([24, 50, 3432])\n"
     ]
    }
   ],
   "source": [
    "class CNN2Transformer(nn.Module):\n",
    "    \"\"\"结合ImageEncoder和TransformerDecoder的完整模型\"\"\"\n",
    "\n",
    "    def __init__(self, embed_size, vocab_size, hidden_size, num_layers, nhead, dim_feedforward, dropout=True):\n",
    "        super(CNN2Transformer, self).__init__()\n",
    "        self.encoder = ImageEncoder(embed_size)\n",
    "        self.decoder = TransformerDecoder(embed_size, vocab_size, num_layers, nhead, dim_feedforward, dropout=0.1 if dropout else 0.0)\n",
    "        self.embed_size = embed_size\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "    def forward(self, images, captions):\n",
    "        \"\"\"\n",
    "        前向传播\n",
    "        Args:\n",
    "            images: Tensor, shape [batch_size, 3, H, W]\n",
    "            captions: Tensor, shape [batch_size, seq_len]\n",
    "        Returns:\n",
    "            outputs: Tensor, shape [batch_size, seq_len, vocab_size]\n",
    "        \"\"\"\n",
    "        features = self.encoder(images)  # [batch_size, embed_size]\n",
    "        outputs = self.decoder(captions, features)  # [batch_size, seq_len, vocab_size]\n",
    "        return outputs\n",
    "\n",
    "    def captionImage(self, image, vocabulary, max_length=50):\n",
    "        \"\"\"生成单张图片的描述\"\"\"\n",
    "        device = image.device\n",
    "        result = []\n",
    "\n",
    "        # 编码图像\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(image)  # [1, embed_size]\n",
    "\n",
    "        # 初始化输入为 <SOS>\n",
    "        inputs = torch.tensor([vocabulary.stoi[\"<SOS>\"]], dtype=torch.long).to(device)  # [1]\n",
    "        inputs = inputs.unsqueeze(0)  # [1, 1]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            outputs = self.decoder(inputs, features)  # [1, seq_len, vocab_size]\n",
    "            outputs = outputs[:, -1, :]  # [1, vocab_size]\n",
    "            _, predicted = outputs.max(1)  # [1]\n",
    "\n",
    "            predicted_word = vocabulary.itos[predicted.item()]\n",
    "            if predicted_word == \"<EOS>\":\n",
    "                break\n",
    "            result.append(predicted_word)\n",
    "            inputs = torch.cat([inputs, predicted.unsqueeze(0)], dim=1)  # [1, seq_len+1]\n",
    "\n",
    "        return result\n",
    "\n",
    "    def captionBatch(self, images, vocabulary, max_length=50):\n",
    "        \"\"\"生成批量图片的描述\"\"\"\n",
    "        device = images.device\n",
    "        batch_size = images.size(0)\n",
    "        results = [[] for _ in range(batch_size)]\n",
    "\n",
    "        # 编码图像\n",
    "        with torch.no_grad():\n",
    "            features = self.encoder(images)  # [batch_size, embed_size]\n",
    "\n",
    "        # 初始化输入为 <SOS>\n",
    "        inputs = torch.tensor([vocabulary.stoi[\"<SOS>\"]] * batch_size, dtype=torch.long).to(device)  # [batch_size]\n",
    "        inputs = inputs.unsqueeze(1)  # [batch_size, 1]\n",
    "\n",
    "        for _ in range(max_length):\n",
    "            outputs = self.decoder(inputs, features)  # [batch_size, seq_len, vocab_size]\n",
    "            outputs = outputs[:, -1, :]  # [batch_size, vocab_size]\n",
    "            _, predicted = outputs.max(1)  # [batch_size]\n",
    "\n",
    "            predicted_words = [vocabulary.itos[p.item()] for p in predicted]\n",
    "            for i, word in enumerate(predicted_words):\n",
    "                if word != \"<EOS>\":\n",
    "                    results[i].append(word)\n",
    "            inputs = torch.cat([inputs, predicted.unsqueeze(1)], dim=1)  # [batch_size, seq_len+1]\n",
    "\n",
    "        return results\n",
    "    \n",
    "# 测试模型\n",
    "print(len(dataset.vocab))\n",
    "model = CNN2Transformer(args.embed_size, len(dataset.vocab), args.hidden_size, args.num_layers, args.nhead, args.dim_feedforward)\n",
    "outputs = model(img_batch, caption_batch)\n",
    "print(outputs.size())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VocabSize: 3432\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 1/1686 [00:02<1:06:25,  2.37s/it, loss=8.35, lr=0.002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([24, 256])\n",
      "targets: tensor([ 9, 21,  7,  ...,  0,  0,  0], device='cuda:0')\n",
      "output:['?', '?', '?', '?', 'down', '?', '?', 'earth', '?', '?', '?', '?', '?', '?', 'earth', '?', '?', '?', '?', 'to', 'earth', 'raises', '?', 'earth', 'earth', 'earth', 'earth', '?', 'earth', 'earth', '?', 'earth', 'earth', '?', 'earth', 'earth', 'earth', 'earth', '?', '?', 'earth', 'earth', '?', '?', '?', '?', '?', 'earth', 'earth', 'instructor', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'festival', 'streaked', 'camels', 'magazine', 'streaked', 'festival', 'festival', 'streaked', 'streaked', 'festival', 'festival', 'festival', 'festival', 'streaked', 'festival', 'festival', 'balck', 'festival', 'streaked', 'streaked', 'streaked', 'festival', 'festival', 'festival', 'festival', 'streaked', 'bungee', 'streaked', 'festival', 'festival', 'streaked', 'streaked', 'streaked', 'streaked', 'streaked', 'festival', 'forest', 'forest', 'forest', 'forest', 'forest', 'carriage', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'fun', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'forest', 'playfully', 'wooded', 'plane', 'quad', 'gallery', 'playfully', 'gallery', 'gallery', 'free', 'somebody', 'wooded', 'playfully', 'somebody', 'wooded', 'carriage', 'quad', 'quad', 'staring', 'quad', 'plane', 'learning', 'staring', 'plane', 'plane', 'strike', 'plane', 'learning', 'learning', 'protesters', 'somebody', 'belt', 'goes', 'gallery', 'learning', 'gallery', 'learning', 'staring', 'playfully', 'somebody', 'plane', 'staring', 'gallery', 'protesters', 'plane', 'strike', 'gallery', 'learning', 'learning', 'goes', 'interesting', 'maneuvers', 'maneuvers', 'wakeboarding', 'shelter', 'interesting', 'shelter', 'upon', 'interesting', 'interesting', 'interesting', 'shelter', 'returns', 'interesting', 'interesting', 'interesting', 'interesting', 'brothers', 'faces', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'involving', 'china', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'china', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'china', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'interesting', 'blurry', 'blurry', 'blurry', 'where', 'blurry', 'very', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'where', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'blurry', 'where', 'blurry', 'blurry', 'where', 'blurry', 'where', 'blurry', 'blurry', 'blurry', 'where', 'blurry', 'where', 'blurry', 'blurry', 'where', 'alike', 'wakeboarding', 'hand', 'hand', 'hand', 'lining', 'lining', 'lining', 'slides', 'hand', 'hand', 'hand', 'hand', 'lining', 'lining', 'lining', 'lining', 'lining', 'lining', 'hand', 'emerges', 'lining', 'lining', 'hand', 'lining', 'lining', 'lining', 'lining', 'hand', 'tied', 'lining', 'lining', 'lining', 'tied', 'hand', 'lining', 'socks', 'lining', 'lining', 'hand', 'lining', 'hand', 'lining', 'hand', 'lining', 'lining', 'tied', 'lining', 'lining', 'camp', 'camp', 'punk', 'camp', 'camp', 'punk', 'camp', 'punk', 'punk', 'camp', 'camp', 'camp', 'punk', 'punk', 'punk', 'camp', 'china', 'walkway', 'excited', 'camp', 'camp', 'camp', 'sooners', 'sooners', 'camp', 'sooners', 'sooners', 'camp', 'punk', 'reads', 'camp', 'camp', 'sooners', 'sooners', 'camp', 'camp', 'punk', 'excited', 'camp', 'china', 'camp', 'camp', 'walkway', 'camp', 'camp', 'punk', 'camp', 'camp', 'sooners', 'bamboo', 'floats', 'floats', 'floats', 'floats', 'tables', 'bamboo', 'leading', 'floats', 'floats', 'floats', 'trophy', 'bamboo', 'floats', 'floats', 'trophy', 'colourful', 'rafts', 'bamboo', 'floats', 'rafts', 'floats', 'escalator', 'floats', 'bamboo', 'size', 'tables', 'bamboo', 'floats', 'floats', 'those', 'colourful', 'floats', 'size', 'nearby', 'bamboo', 'size', 'patches', 'bamboo', 'size', 'size', 'size', 'size', 'floats', 'floats', 'size', 'size', 'size', 'floats', 'island', 'barriers', 'winner', 'walked', 'island', 'pounces', 'herd', 'island', 'walked', 'island', 'walked', 'herd', 'island', 'island', 'celebrate', 'winner', 'herd', 'island', 'walked', 'herd', 'walked', 'pounces', 'pounces', 'waterskier', 'pounces', 'cellphone', 'island', 'walked', 'pounces', 'winner', 'island', 'island', 'pounces', 'winner', 'walked', 'pounces', 'pounces', 'post', 'pounces', 'walked', 'pounces', '5', 'walked', 'thin', 'pounces', 'pounces', 'pounces', 'pounces', 'thin', 'necklace', 'indoors', 'flies', 'flies', 'traditional', 'flies', 'indoors', 'indoors', 'indoors', 'fingers', 'necklace', 'necklace', 'strange', 'indoors', 'indoors', 'indoors', 'indoors', 'indoors', 'indoors', 'necklace', 'indoors', 'attempting', 'necklace', 'necklace', 'fingers', 'indoors', 'necklace', 'necklace', 'necklace', 'flies', 'flies', 'flies', 'attempting', 'indoors', 'flies', 'necklace', 'indoors', 'necklace', 'necklace', 'indoors', 'necklace', 'necklace', 'indoors', 'attempting', 'earphones', 'indoors', 'necklace', 'indoors', 'flies', 'camera', 'camera', 'skinny', 'camera', 'camera', 'finish', 'finish', 'finish', 'finish', 'camera', 'camera', 'finish', 'camera', 'camera', 'camera', 'camera', 'camera', 'camera', 'kisses', 'expressions', 'camera', 'kisses', 'expressions', 'camera', 'camera', 'finish', 'finish', 'camera', 'camera', 'camera', 'kisses', 'camera', 'camera', 'kisses', 'kisses', 'pounces', 'camera', 'camera', 'pounces', 'kisses', 'kisses', 'camera', 'kisses', 'kisses', 'camera', 'finish', 'kisses', 'finish', 'kisses', 'campsite', 'artificial', 'which', 'weeds', 'weeds', 'weeds', 'which', 'which', 'frozen', 'weeds', 'weeds', 'weeds', 'buses', 'shephard', 'buses', 'buses', 'shephard', 'shephard', 'which', 'shephard', 'earth', 'earth', 'weeds', 'shephard', 'shephard', 'weeds', 'earth', 'earth', 'earth', 'weeds', 'earth', 'earth', 'weeds', 'weeds', 'earth', 'earth', 'earth', 'shephard', 'shephard', 'earth', 'weeds', 'earth', 'shephard', 'weeds', 'shephard', 'earth', 'shephard', 'earth', 'earth', 'hopscotch', 'pool', 'hand', 'great', 'hand', 'great', 'great', 'greenish', 'pool', 'hand', 'hand', 'hand', 'hand', 'front', 'i', 'great', 'great', 'pool', 'front', 'front', 'hand', 'front', 'alley', 'great', 'great', 'pool', 'weeds', 'hand', 'weeds', 'pad', 'pool', 'hand', 'great', 'pool', 'great', 'front', 'great', 'pad', 'weeds', 'pad', 'front', 'shade', 'hand', 'front', 'great', 'hand', 'weeds', 'pool', 'alley', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'toboggan', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'pirates', 'rabbit', 'toboggan', 'shelter', 'rabbit', 'shelter', 'rabbit', 'pirates', 'pirates', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'pirates', 'pirates', 'pirates', 'rabbit', 'pirates', 'rabbit', 'rabbit', 'shelter', 'rabbit', 'rabbit', 'rabbit', 'pirates', 'pirates', 'pirates', 'stair', 'stair', 'stair', 'tool', 'tool', 'paintball', 'creature', 'stair', 'tattoo', 'tool', 'goes', 'stair', 'cookie', 'cookie', 'huts', 'strike', 'strike', 'cookie', 'males', 'strike', 'goes', 'stair', 'strike', 'goes', 'cookie', 'strike', 'cookie', 'strike', 'shirted', 'cookie', 'strike', 'goes', 'strike', 'strike', 'cookie', 'strike', 'males', 'strike', 'bend', 'strike', 'strike', 'shirted', 'strike', 'goes', 'cookie', 'strike', 'strike', 'goes', 'goes', 'golf', 'tattooed', 'scuba', 'tattooed', 'military', 'scuba', 'flops', 'tattooed', 'scuba', 'tattooed', 'container', 'scuba', 'tattooed', 'chunk', 'scuba', 'scuba', 'streaked', 'scuba', 'lift', 'drives', 'strike', 'strike', 'lift', 'strike', 'scuba', 'tattooed', 'tattooed', 'tattooed', 'tattooed', 'strike', 'streaked', 'trampoline', '&', 'tattooed', 'streaked', 'tattooed', 'lift', 'tattooed', 'streaked', 'goatee', 'streaked', 'lift', 'lift', 'streaked', 'strike', 'tattooed', 'drives', 'scuba', 'streaked', 'paper', 'fist', 'paper', 'paper', 'paper', 'paper', 'paper', 'paper', 'paper', 'paper', 'paper', 'chopsticks', 'paper', 'control', 'interested', 'boxes', 'interested', 'paper', 'paper', 'monument', 'pro', 'pro', 'monument', 'half', 'monument', 'pro', 'monument', 'pro', 'half', 'monument', 'monument', 'pro', 'half', 'monument', 'monument', 'half', 'paper', 'pro', 'monument', 'half', 'pro', 'upset', 'pro', 'monument', 'paper', 'monument', 'monument', 'chopsticks', 'pro', 'glides', 'hand', 'n', 'winding', 'glides', 'lollipop', 'pillow', 'n', 'n', 'winding', 'n', 'n', 'seal', 'seal', 'like', 'winding', 'lollipop', 'n', 'winding', 'earring', 'n', 'lollipop', 'n', 'n', 'n', 'n', 'seal', 'n', 'n', 'n', 'winding', 'lollipop', 'hand', 'n', 'lollipop', 'n', 'lollipop', 'n', 'n', 'lollipop', 'n', 'very', 'n', 'onstage', 'earring', 'very', 'n', 'n', 'winding', 'collared', 'wall', 'collared', 'spray', 'collared', 'view', 'spray', 'view', 'collared', 'swims', 'collared', 'collared', 'collared', 'spray', 'collared', 'collared', 'collared', 'collared', 'collared', 'collared', 'six', 'camp', 'collared', 'collared', 'spray', 'collared', 'collared', 'collared', 'collared', 'collared', 'sumo', 'collared', 'collared', 'view', 'sumo', 'collared', 'sumo', 'view', 'collared', 'sumo', 'collared', 'collared', 'sumo', 'collared', 'collared', 'collared', 'collared', 'spray', 'collared', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'defenders', 'rabbit', 'defenders', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'defenders', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'defenders', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'rabbit', 'fair', 'goggles', 'colourful', 'fair', 'feild', 'goggles', 'training', 'colourful', 'goggles', 'groucho', 'goggles', 'colourful', 'colourful', 'goggles', 'embrace', 'goggles', 'goggles', 'goggles', 'colourful', 'teenager', 'goggles', 'goggles', 'goggles', 'colourful', 'colourful', 'goggles', 'goggles', 'colourful', 'goggles', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'goggles', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'colourful', 'goggles', 'colourful', 'goggles', 'goggles', 'colourful', 'squatting', 'squatting', 'squatting', 'rabbit', 'squatting', 'tagged', 'tabby', 'crowd', 'camp', 'rabbit', 'see', 'squatting', 'squatting', 'rabbit', 'trampoline', 'squatting', 'see', 'canal', 'canal', 'canal', 'view', 'khaki', 'see', 'see', 'court', 'squatting', 'resting', 'see', 'animal', 'resting', 'cream', 'horizontal', 'animal', 'cream', 'horizontal', 'horizontal', 'tabby', 'horizontal', 'horizontal', 'cars', 'entrance', 'tabby', 'horizontal', 'horizontal', 'entrance', 'horizontal', 'entrance', 'squatting', 'numbers', 'what', 'view', 'letter', 'view', 'thin', 'view', 'celebrate', 'view', 'letter', 'view', 'thin', 'view', 'thin', 'thin', 'view', 'thin', 'view', 'view', 'thin', 'thin', 'thin', 'thin', 'thin', 'thin', 'thin', 'view', 'view', 'thin', 'view', 'view', 'thin', 'thin', 'view', 'thin', 'thin', 'thin', 'thin', 'view', 'thin', 'thin', 'thin', 'thin', 'thin', 'thin', 'thin', 'thin', 'view', 'thin', 'thin']\n",
      "torch.Size([24, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 3/1686 [00:02<18:59,  1.48it/s, loss=6.98, lr=0.002]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([  4, 530, 301,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 9,  7, 18,  ...,  0,  0,  0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 4, 22, 94,  ...,  0,  0,  0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   0%|          | 7/1686 [00:02<06:37,  4.23it/s, loss=6.47, lr=0.002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([174, 121,  32,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([514,  22,  18,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', '<EOS>', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', '<EOS>', '.', '.', '.', '.', 'a', 'a', '.', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '<EOS>', 'a', 'a', '.', '.', '.', '.', '.', '.', '.', '<EOS>', '.', '.', 'a', '<EOS>', 'a', '.', '.', '.', '.', 'a', '<EOS>', 'a', '<EOS>', 'a', '.', '<EOS>', 'a', '.', 'a', '.', '.', 'a', '.', '.', '.', '.', '.', '<EOS>', 'a', '.', '<EOS>', '.', 'a', 'a', '.', '.', '<EOS>', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', '<EOS>', '.', '.', '.', 'a', '.', '.', 'a', '<EOS>', '.', 'a', '.', '<EOS>', '.', 'a', 'a', '.', 'a', '.', '.', 'a', '.', 'a', '.', '.', '.', '.', '<EOS>', 'a', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', '.', 'a', '.', '.', 'a', '.', 'a', '<EOS>', '.', '<EOS>', '<EOS>', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', '.', '.', 'a', 'a', '.', '.', '.', '.', '.', '.', '.', 'a', '.', '.', '.', 'a', '.', '.', '.', 'a', '.', '.', '.', '.', '.', '.', '<EOS>', '.', '.', '.', '.', '.', '.', 'a', 'a', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', '.', 'a', '.', 'a', 'a', 'a', '<EOS>', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', '.', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', '<EOS>', 'a', 'a', '<EOS>', 'a', '.', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', '.', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '<EOS>', '.', '.', '.', 'a', '.', '.', '<EOS>', '<EOS>', '<EOS>', '.', '<EOS>', '.', 'a', '<EOS>', '.', '.', '.', 'a', '.', 'a', '.', '.', '.', '.', '.', '<EOS>', '.', '.', '<EOS>', 'a', '.', 'a', '.', '<EOS>', '.', 'a', '.', '.', '.', 'a', '.', 'a', '.', 'a', '.', '.', 'a', '<EOS>', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', 'a', '<EOS>', '.', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', '.', '.', 'a', 'a', 'a', '<EOS>']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 70,  87, 746,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', '.', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', '.', '.', 'a', 'a', '.', 'a', '.', '.', '.', '.', '.', 'a', '.', '.', '.', '.', 'a', '.', 'a', 'a', '.', '.', '.', '.', '.', '.', '.', 'a', 'a', '.', '.', 'a', '.', '.', '.', '.', '.', 'a', '.', '.', 'a', '.', 'a', '.', '.', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '<EOS>', '.', 'a', 'a', 'a', '.', '.', '.', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', '.', 'a', '.', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', 'a', '.', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', '.', '.', '.', '.', 'a', 'a', 'a', '.', '.', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', '.', '.', 'a', 'a', '.', '.', 'a', '.', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', '.', '.', 'a', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', '.', '.', 'a', 'a', '.', 'a', 'a', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', '.', '.', '.', '.', 'a', 'a', '.', 'a', '.', 'a', '.', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', '.', 'a', '.', 'a', 'a', 'a', '.', 'a', 'a', '.', 'a', '<EOS>', '.', '.', '.', '.', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', '.', 'a', 'a', 'a', '.', '.', '.', 'a', '.', '.', '.', '.', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', '.', '.', 'a', '.', '.', 'a', '.', '.', '.', '.']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 4, 21,  7,  ...,  0,  0,  0], device='cuda:0')\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   1%|          | 9/1686 [00:02<04:52,  5.73it/s, loss=6.14, lr=0.002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', '.', '.', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', '.', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', 'a', '.', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '.', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 4, 12,  6,  ...,  0,  0,  0], device='cuda:0')\n",
      "output:['.', '.', 'the', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'the', '<EOS>', '<EOS>', '<EOS>', 'the', 'a', 'a', 'the', '<EOS>', '<EOS>', '<EOS>', 'a', 'the', '<EOS>', 'a', 'the', '.', 'the', 'in', 'a', 'the', 'a', 'a', '<EOS>', '.', 'a', 'the', '<EOS>', '<EOS>', 'a', 'the', 'the', 'the', '.', 'the', '<EOS>', '<EOS>', '<EOS>', 'in', 'the', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '.', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '.', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '.', '.', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', '<EOS>', 'a', 'a', '.', '.', 'a', '.', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([ 9,  7, 18,  ...,  0,  0,  0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'in', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '.', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'in', 'in', 'a', '<EOS>', 'a', 'a', 'a', 'in', 'a', '<EOS>', 'the', 'the', '.', 'a', 'a', 'a', 'a', 'a', 'in', '<EOS>', 'a', 'the', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'the', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'in', '<EOS>', '<EOS>', 'the', 'a', 'a', 'in', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'in', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'the', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'in', 'a', 'in', 'a', 'a', '<EOS>', '<EOS>', 'in', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'in', 'a', '<EOS>', 'in', 'a', 'the', 'a', 'a', 'in', 'in', 'in', 'in', 'a', '<EOS>', 'in', 'a', 'the', '<EOS>', 'in', 'a', '<EOS>', 'the', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'the', 'in', 'in', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '.', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'the', 'a', 'a', 'a', 'a', 'the', 'a', 'a', 'a', 'the', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'the', 'the', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'in', 'the', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'the', 'in', 'a', '<EOS>', 'the', 'in', '<EOS>', 'a', 'in', 'a', 'in', 'a', 'a', 'in', 'the', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'the', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'the', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'in', 'a', 'in', 'a', '<EOS>', 'a', 'in', 'a', 'a', 'the', 'in', '<EOS>', 'a', 'a', 'in', '<EOS>', '<EOS>', 'a', 'in', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   1%|          | 11/1686 [00:03<03:55,  7.12it/s, loss=5.99, lr=0.002]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "targets: tensor([  4, 112, 780,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'the', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'in', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'in', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'in', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'in', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'in', '<EOS>', '.', '<EOS>', 'a', '<EOS>', 'in', 'a', 'in', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'in', 'a', 'a', '<EOS>', 'a', 'in', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'in', 'a', 'a', 'in', 'a', 'a', 'a', '.', '<EOS>', 'in', '<EOS>', 'a', '<EOS>', '<EOS>', 'the', 'a', 'in', '.', '<EOS>', '<EOS>', 'in', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'in', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'in', '<EOS>', '<EOS>', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'in', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', '.', 'a', 'in', '<EOS>', 'a', '<EOS>', 'in', 'in', 'a', 'a', 'in', 'a', 'in', 'in', 'in', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'in', 'in', '<EOS>', 'a', 'in', 'in', '<EOS>', 'in', '<EOS>', 'a', 'a', '<EOS>', 'in', 'a', 'in', 'a', 'in', 'in', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'in', '<EOS>', '.', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'the', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'in', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'in', '<EOS>', 'a', '<EOS>', 'a', 'in', 'a', 'in', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'in', 'a', 'in', 'a', 'a', '<EOS>', 'in', 'a', '<EOS>', '<EOS>', 'in', '<EOS>', 'in', '<EOS>', '<EOS>', '<EOS>', 'in', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'in', 'in', 'a', 'in', 'a', '<EOS>', 'a', '<EOS>', 'in', 'in', 'a', 'in', '<EOS>', '<EOS>', 'in', 'a', 'in', '<EOS>', 'a', 'in', 'a', 'a', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'in', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'in', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'in', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'in', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'in', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'in', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a']\n",
      "torch.Size([24, 256])\n",
      "targets: tensor([487,   8, 160,  ...,   0,   0,   0], device='cuda:0')\n",
      "output:['a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'in', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'in', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'in', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'in', 'a', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '.', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '.', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', '<EOS>', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', '<EOS>', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', '<EOS>', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'a', 'in', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', '.', 'a', 'a', 'a', 'a', 'a', 'a', 'a', '<EOS>', 'a', 'a', 'a']\n",
      "torch.Size([24, 256])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training Epoch 0:   1%|          | 12/1686 [00:03<08:10,  3.41it/s, loss=5.99, lr=0.002]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[74], line 100\u001b[0m\n\u001b[1;32m     98\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mStepLR(optimizer, args\u001b[38;5;241m.\u001b[39mwhen_decay, \u001b[38;5;241m0.1\u001b[39m)\n\u001b[1;32m     99\u001b[0m filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcaption_model_lr\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_decay\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m_bsize\u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.pth.tar\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(args\u001b[38;5;241m.\u001b[39mlr, args\u001b[38;5;241m.\u001b[39mwhen_decay, args\u001b[38;5;241m.\u001b[39mbatch_size)\n\u001b[0;32m--> 100\u001b[0m \u001b[43mtrain_and_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvocab\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[74], line 18\u001b[0m, in \u001b[0;36mtrain_and_eval\u001b[0;34m(args, model, train_loader, val_loader, optimizer, scheduler, filename, vocab)\u001b[0m\n\u001b[1;32m     15\u001b[0m captions \u001b[38;5;241m=\u001b[39m captions\u001b[38;5;241m.\u001b[39mto(args\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# 前向传播，注意我们传入 captions 的前一部分\u001b[39;00m\n\u001b[0;32m---> 18\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimgs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len-1, vocab_size]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     21\u001b[0m \u001b[38;5;66;03m# 目标是 captions 的后一部分\u001b[39;00m\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[49], line 21\u001b[0m, in \u001b[0;36mCNN2Transformer.forward\u001b[0;34m(self, images, captions)\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;124;03m前向传播\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;124;03m    outputs: Tensor, shape [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     20\u001b[0m features \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(images)  \u001b[38;5;66;03m# [batch_size, embed_size]\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaptions\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# [batch_size, seq_len, vocab_size]\u001b[39;00m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m outputs\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1553\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1551\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1552\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1553\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/myconda/lib/python3.12/site-packages/torch/nn/modules/module.py:1562\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1557\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1558\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1559\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1560\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1561\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1562\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1564\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1565\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[73], line 48\u001b[0m, in \u001b[0;36mTransformerDecoder.forward\u001b[0;34m(self, captions, memory)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;66;03m# 将memory扩展为与Transformer期望的形状一致 [1, batch_size, embed_size]\u001b[39;00m\n\u001b[1;32m     46\u001b[0m memory \u001b[38;5;241m=\u001b[39m memory\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 48\u001b[0m tgt_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_square_subsequent_mask\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcaption_emb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;66;03m# (caption_emb.size(), memory.size(), tgt_mask.size())\u001b[39;00m\n\u001b[1;32m     50\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtransformer_decoder(caption_emb, memory, tgt_mask\u001b[38;5;241m=\u001b[39mtgt_mask)  \u001b[38;5;66;03m# [seq_len, batch_size, embed_size]\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "def train_and_eval(args, model, \n",
    "                  train_loader, val_loader,\n",
    "                  optimizer, scheduler, filename, vocab):\n",
    "    criterion = torch.nn.CrossEntropyLoss(ignore_index=vocab.stoi[\"<PAD>\"])  # 忽略填充项的损失\n",
    "    best_loss = 1e5\n",
    "    best_bleu = 0.0\n",
    "    for epoch in range(args.num_epochs):\n",
    "        model.train()\n",
    "        _loss = 0\n",
    "        _total = 0\n",
    "\n",
    "        flickr_loader = tqdm(train_loader, desc=\"Training Epoch {}\".format(epoch), total=len(train_loader))\n",
    "        for imgs, captions in flickr_loader:\n",
    "            imgs = imgs.to(args.device)\n",
    "            captions = captions.to(args.device)\n",
    "\n",
    "            # 前向传播，注意我们传入 captions 的前一部分\n",
    "            outputs = model(imgs, captions[:, :-1])  # [batch_size, seq_len-1, vocab_size]\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            # 目标是 captions 的后一部分\n",
    "            targets = captions[:, 1:]  # [batch_size, seq_len-1]\n",
    "\n",
    "            # 计算损失\n",
    "            outputs = outputs.reshape(-1, outputs.size(2))  # [(batch_size * (seq_len-1)), vocab_size]\n",
    "            targets = targets.reshape(-1)  # [(batch_size * (seq_len-1))]\n",
    "            print(f'targets: {targets}')\n",
    "            print(f'output:{[dataset.vocab.itos[idx.item()] for idx in outputs.argmax(dim=1)]}')\n",
    "\n",
    "            loss = criterion(outputs, targets)\n",
    "            loss.backward()\n",
    "\n",
    "            _loss += loss.item() * targets.size(0)\n",
    "            _total += targets.size(0)\n",
    "            optimizer.step()\n",
    "            flickr_loader.set_postfix(loss=_loss / _total, lr=optimizer.param_groups[0]['lr'])\n",
    "        scheduler.step()\n",
    "        _loss /= _total\n",
    "\n",
    "        model.eval()\n",
    "        smoothing_fn = SmoothingFunction().method1\n",
    "        references, hypotheses = [], []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            eval_loader = tqdm(val_loader, desc=\"Evaluation\", total=len(val_loader))\n",
    "            for imgs, captions in eval_loader:\n",
    "                imgs = imgs.to(args.device)\n",
    "                captions = captions.to(args.device)\n",
    "\n",
    "                generated_captions = model.captionBatch(imgs, vocab)\n",
    "                for i in range(len(imgs)):\n",
    "                    ref = captions[i].tolist()\n",
    "                    references.append([[vocab.itos[idx] for idx in ref if idx not in {vocab.stoi[\"<SOS>\"], vocab.stoi[\"<EOS>\"], vocab.stoi[\"<PAD>\"]}]])\n",
    "                    hypotheses.append(generated_captions[i])\n",
    "\n",
    "        # 计算 BLEU 分数\n",
    "        avg_bleu = corpus_bleu(references, hypotheses, smoothing_function=smoothing_fn)\n",
    "        print(f\"Validation Set Average BLEU Score: {avg_bleu:.4f}\")\n",
    "\n",
    "        print(f\"Loss for epoch {epoch}: {_loss}\")\n",
    "        if _loss < best_loss:\n",
    "            best_loss = _loss\n",
    "            checkpoint = {\n",
    "                \"state_dict\": model.state_dict(),\n",
    "                \"optimizer\": optimizer.state_dict(),\n",
    "                \"scheduler\": scheduler.state_dict(),\n",
    "            }\n",
    "            save_checkpoint(checkpoint, filename)\n",
    "\n",
    "def save_checkpoint(state, filename):\n",
    "    print(\"saving checkpoint!\")\n",
    "    torch.save(state, filename)\n",
    "\n",
    "def load_checkpoint(name, model):\n",
    "    print(\"loading checkpoint!\")\n",
    "    checkpoint = torch.load(name, map_location=model.device if hasattr(model, 'device') else 'cpu')\n",
    "    model.load_state_dict(checkpoint[\"state_dict\"])\n",
    "\n",
    "args.vocab_size = len(dataset.vocab)\n",
    "print(\"VocabSize:\", args.vocab_size)\n",
    "args.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = CNN2Transformer(\n",
    "        embed_size=args.embed_size,\n",
    "        vocab_size=args.vocab_size,\n",
    "        hidden_size=args.embed_size,\n",
    "        num_layers=args.num_layers,\n",
    "        nhead=args.nhead,\n",
    "        dim_feedforward=args.dim_feedforward,\n",
    "        dropout=True\n",
    "    ).to(device=args.device)\n",
    "\n",
    "if args.optim.lower() == 'adamw':\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr)\n",
    "else:\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, args.when_decay, 0.1)\n",
    "filename = \"caption_model_lr{}_decay{}_bsize{}.pth.tar\".format(args.lr, args.when_decay, args.batch_size)\n",
    "train_and_eval(args, model, loader, val_loader, optimizer, scheduler, filename, dataset.vocab)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myconda",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
